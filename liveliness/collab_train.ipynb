{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw5Ty9zHO02e"
      },
      "outputs": [],
      "source": [
        "!pip install face_recognition\n",
        "!pip install tensorflow\n",
        "!pip3 install retina-face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2ydP8UwmN61"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/NVIDIA/apex && cd apex\n",
        "# !python /content/apex/setup.py install\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToYrVLQAJ4iZ"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1-GY1pmK2hP5GGWZJQsj59kJ33tnnjdSM\n",
        "!unzip /content/drive/MyDrive/liveliness/dataset.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpJ5ETDlldBG"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/liveliness/dataset_faces /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/drive/MyDrive/liveliness/real/* /content/drive/MyDrive/liveliness/dataset_faces/real/"
      ],
      "metadata": {
        "id": "muM12kpSt_vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-RkHXxvJiYe",
        "outputId": "11c1790f-36ef-4a2b-9853-56f91bc7c7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "# import face_recognition as rec\n",
        "# from retinaface import RetinaFace\n",
        "\n",
        "# import the necessary packages\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import RandomFlip\n",
        "from tensorflow.keras.layers import RandomRotation\n",
        "from tensorflow.keras.layers import RandomZoom\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.layers import Rescaling\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLQKQLXgJk90"
      },
      "outputs": [],
      "source": [
        "img_height = 32\n",
        "img_width = 32\n",
        "\n",
        "# data_augmentation = Sequential(\n",
        "#   [\n",
        "#     RandomFlip(\"horizontal\",\n",
        "#                       input_shape=(img_height,\n",
        "#                                   img_width,\n",
        "#                                   3)),\n",
        "#     RandomRotation(0.1),\n",
        "#     RandomZoom(0.15),\n",
        "#   ]\n",
        "# )\n",
        "\n",
        "class LivenessNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t#adding data augmentation\n",
        "\t\t# model.add(data_augmentation)\n",
        "\t\t    \n",
        "    # first CONV => RELU => CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(16, (3, 3), padding=\"same\", input_shape=inputShape))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# second CONV => RELU => CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.2))\n",
        "\n",
        "        # first (and only) set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(64))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As_pwUUSdLTi"
      },
      "source": [
        "Attemp to pick images from directory itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZwTYqOJpd1"
      },
      "outputs": [],
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "num = \"44\"\n",
        "\n",
        "# initialize the initial learning rate, batch size, and number of\n",
        "# epochs to train for\n",
        "batch_size = 64\n",
        "BS = 16\n",
        "EPOCHS = 400\n",
        "INIT_LR = 1e-4\n",
        "DEFAULT_CONFIDENCE = 0.5\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "data_dir = Path(\"/content/drive/MyDrive/liveliness/dataset_faces/\")\n",
        "!rm -rf /content/drive/MyDrive/liveliness/dataset_faces/.ipynb_checkpoints\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)\n",
        "list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n",
        "val_size = int(image_count * 0.2)\n",
        "\n",
        "train_ds = image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "normalization_layer = Rescaling(1./255)\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "\n",
        "num_classes = len(class_names)\n",
        "print(\"Class names are: \", class_names)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model = LivenessNet.build(width=32, height=32, depth=3, classes=num_classes)\n",
        "\n",
        "# model = load_model(\"liveness.model29\")\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "steps = len(train_ds) // BS\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=EPOCHS\n",
        ")\n",
        "\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "#plotting and saving results\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, EPOCHS), loss, label=\"train_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), val_loss, label=\"val_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), acc, label=\"train_acc\")\n",
        "plt.plot(np.arange(0, EPOCHS), val_acc, label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"/content/result/plot\" + num + \".png\")\n",
        "\n",
        "#saving model\n",
        "print(\"[INFO] serializing network to '{}'...\".format(\"liveness.model\" + num))\n",
        "model.save(\"/content/result/liveness.model\" + num, save_format=\"h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb46vOJieIJt"
      },
      "source": [
        "USING OLD TYPE OF TRAININg TECHNIQUE AS PER PYIMAGESEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS0aYuJHLVBN"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/liveliness/dataset_faces/ /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ICCf4BHeNd-"
      },
      "outputs": [],
      "source": [
        "num = \"51\"\n",
        "\n",
        "# initialize the initial learning rate, batch size, and number of\n",
        "# epochs to train for\n",
        "INIT_LR = 1e-4\n",
        "BS = 192\n",
        "EPOCHS = 1250\n",
        "DEFAULT_CONFIDENCE = 0.5\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(\"/content/dataset_faces\"))\n",
        "data = []\n",
        "labels = []\n",
        "# loop over all image paths\n",
        "for imagePath in tqdm(imagePaths):\n",
        "\t#grab face ROI\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\tface = cv2.imread(imagePath)\n",
        "\n",
        "\t# update the data and labels lists, respectively\n",
        "\tshape = face.shape\n",
        "\tif (shape[0] > 0 and shape[1] > 0):\n",
        "\t\tdata.append(face)\n",
        "\t\tlabels.append(label)\n",
        "    \n",
        "# convert the data into a NumPy array, then preprocess it by scaling\n",
        "# all pixel intensities to the range [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "\n",
        "# encode the labels (which are currently strings) as integers and then\n",
        "# one-hot encode them\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = to_categorical(labels, 2)\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model = LivenessNet.build(width=32, height=32, depth=3,\n",
        "\tclasses=len(le.classes_))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
        "H = model.fit(x=aug.flow(trainX, trainY, batch_size=BS),\n",
        "\tvalidation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX, batch_size=BS)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=le.classes_))\n",
        "\n",
        "# save the network to disk\n",
        "print(\"[INFO] serializing network to '{}'...\".format(\"liveness.model\" + num))\n",
        "model.save(\"/content/result/liveness.model\" + num, save_format=\"h5\")\n",
        "# save the label encoder to disk\n",
        "f = open(\"/content/result/le\" + num + \".pickle\", \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"/content/result/plot\" + num + \".png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTtN8pR5SKP"
      },
      "source": [
        "Moving results to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWkCYKNbJLVD"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/liveliness/semiset\n",
        "# !zip -F semiSplit.zip --out semiset.zip\n",
        "# !unzip semiset.zip -d /content/drive/MyDrive/liveliness/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bh_DNR65Fqy"
      },
      "outputs": [],
      "source": [
        "!cp -r result /content/drive/MyDrive/liveliness/results/result51"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPjaPXsTWJN2"
      },
      "outputs": [],
      "source": [
        "!rm -rf result/\n",
        "!mkdir result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwnFcXxU281p"
      },
      "source": [
        "Code to cut and save faces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgueAH9lPp8R",
        "outputId": "51d42ecf-bf9d-4367-f98f-9a82828f4da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading face detector...\n",
            "[INFO] loading images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 87/1002 [00:31<10:09,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 228/1002 [01:57<05:31,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 244/1002 [02:03<05:09,  2.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by other detectors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 353/1002 [03:00<09:44,  1.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 360/1002 [03:06<13:15,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 362/1002 [03:08<10:07,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 703/1002 [04:36<00:56,  5.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 760/1002 [04:43<00:46,  5.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 831/1002 [04:56<00:36,  4.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 926/1002 [05:11<00:14,  5.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out of bounds by first, no detections found by second detector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1002/1002 [05:22<00:00,  3.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# load our serialized face detector from disk\n",
        "print(\"[INFO] loading face detector...\")\n",
        "protoPath = \"/content/drive/MyDrive/liveliness/face_detector/deploy.prototxt\"\n",
        "modelPath = \"/content/drive/MyDrive/liveliness/face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "# protoPath = \"/Users/ruchit/Imperial/livelinessDetection/ann_approach/livenet/livenessnet/face_detector/deploy.prototxt\"\n",
        "# modelPath = \"/Users/ruchit/Imperial/livelinessDetection/ann_approach/livenet/livenessnet/face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(\"/content/semiset/jpg\"))\n",
        "data = []\n",
        "labels = []\n",
        "# loop over all image paths\n",
        "for imagePath in tqdm(imagePaths):\n",
        "\t# extract the class label from the filename, load the image and\n",
        "\t# resize it to be a fixed 32x32 pixels, ignoring aspect ratio\n",
        "\tsplit = imagePath.split(os.path.sep)\n",
        "\tlabel = split[-2]\n",
        "\timage = cv2.imread(imagePath)\n",
        "\n",
        "\t# grab the frame dimensions and construct a blob from the frame\n",
        "\t(h, w) = image.shape[:2]\n",
        "\t#TODO: make 224\n",
        "\tblob = cv2.dnn.blobFromImage(cv2.resize(image, (224, 224)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "\n",
        "\t# pass the blob through the network and obtain the detections and\n",
        "\t# predictions\n",
        "\tnet.setInput(blob)\n",
        "\tdetections = net.forward()\n",
        "\n",
        "\t# ensure at least one face was found\n",
        "\tif len(detections) > 0:\n",
        "\t\t# we're making the assumption that each image has only ONE\n",
        "\t\t# face, so find the bounding box with the largest probability\n",
        "\t\ti = np.argmax(detections[0, 0, :, 2])\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# ensure that the detection with the largest probability also\n",
        "\t\t# means our minimum probability test (thus helping filter out\n",
        "\t\t# weak detections\n",
        "\t\tif confidence > DEFAULT_CONFIDENCE:\n",
        "    \t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the face and extract the face ROI\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\t\t\tif (endX > w or endY > h):\n",
        "\t\t\t\timg = rec.load_image_file(imagePath)\n",
        "\t\t\t\tface_locations = rec.face_locations(img)\n",
        "\n",
        "\t\t\t\tif (len(face_locations) == 0):\n",
        "\t\t\t\t\tprint(\"out of bounds by first, no detections found by second detector\")\n",
        "\t\t\t\t\tcontinue\n",
        "    \t    #applying third detector\n",
        "\t\t\t\t\tresp = RetinaFace.detect_faces(image)\n",
        "\t\t\t\t\tif (len(resp) == 0):\n",
        "\t\t\t\t\t\tprint(\"out of bounds by first, no detections found by other detectors\")\n",
        "\t\t\t\t\t\tcontinue  \n",
        "\t\t\t\t\tconf = 0\n",
        "\t\t\t\t\tfaceRect = []\n",
        "\t\t\t\t\tfor item in resp:\n",
        "\t\t\t\t\t\tresponse = resp[item]\n",
        "\t\t\t\t\t\tif (response['score'] > conf):\n",
        "\t\t\t\t\t\t\tconf = response['score']\n",
        "\t\t\t\t\t\t\tfaceRect = response['facial_area']\n",
        "\t\t\t\t\tif (conf > DEFAULT_CONFIDENCE):\n",
        "\t\t\t\t\t\tstartX, startY, endX, endY = faceRect\n",
        "\t\t\t\t\telse: \n",
        "\t\t\t\t\t\tprint(\"out of bounds by first, no detections found by other detectors\")\n",
        "\t\t\t\t\t\tcontinue  \n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tstartY, endX, endY, startX = face_locations[0]\n",
        "\t\telse:\n",
        "\t\t\t# confidence was low\n",
        "\t\t\timg = rec.load_image_file(imagePath)\n",
        "\t\t\tface_locations = rec.face_locations(img)\n",
        "\t\t\tif (len(face_locations) == 0):\n",
        "\t\t\t\tprint(\"out of bounds by first, no detections found by other detectors\")\n",
        "\t\t\t\tcontinue\n",
        "\t\t\t\t#applying third detector\n",
        "\t\t\t\tresp = RetinaFace.detect_faces(image)\n",
        "\t\t\t\tif (len(resp) == 0):\n",
        "\t\t\t\t\tprint(\"out of bounds by first, no detections found by other detectors\")\n",
        "\t\t\t\t\tcontinue  \n",
        "\t\t\t\tconf = 0\n",
        "\t\t\t\tfaceRect = []\n",
        "\t\t\t\tfor item in resp:\n",
        "\t\t\t\t\tresponse = resp[item]\n",
        "\t\t\t\t\tif (response['score'] > conf):\n",
        "\t\t\t\t\t\tconf = response['score']\n",
        "\t\t\t\t\t\tfaceRect = response['facial_area']\n",
        "\n",
        "\t\t\t\tif (conf > DEFAULT_CONFIDENCE):\n",
        "\t\t\t\t\tstartX, startY, endX, endY = faceRect\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tprint(\"out of bounds by first, no detections found by other detectors\")\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\telse:\n",
        "\t\t\t\tstartY, endX, endY, startX = face_locations[0]\n",
        "\telse:\n",
        "\t\t#no detection by first\n",
        "\t\timg = rec.load_image_file(imagePath)\n",
        "\t\tface_locations = rec.face_locations(img)\n",
        "\t\tif (len(face_locations) == 0):\n",
        "\t\t\tprint(\"no detections\")\n",
        "\t\t\tcontinue\n",
        "\t\t\t#applying third detector\n",
        "\t\t\tresp = RetinaFace.detect_faces(image)\n",
        "\t\t\tif (len(resp) == 0):\n",
        "\t\t\t\tprint(\"no detections\")\n",
        "\t\t\t\tcontinue  \n",
        "\t\t\tconf = 0\n",
        "\t\t\tfaceRect = []\n",
        "\t\t\tfor item in resp:\n",
        "\t\t\t\tresponse = resp[item]\n",
        "\t\t\t\tif (response['score'] > conf):\n",
        "\t\t\t\t\tconf = response['score']\n",
        "\t\t\t\t\tfaceRect = response['facial_area']\n",
        "\n",
        "\t\t\tif (conf > DEFAULT_CONFIDENCE):\n",
        "\t\t\t\tstartX, startY, endX, endY = faceRect\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"no detections\")\n",
        "\t\t\t\tcontinue\n",
        "\t\telse:\n",
        "\t\t\tstartY, endX, endY, startX = face_locations[0]\n",
        "\n",
        "\t#grab face ROI\n",
        "\tface = image[startY:endY, startX:endX]\n",
        "\t\n",
        "\t# update the data and labels lists, respectively\n",
        "\tshape = face.shape\n",
        "\tif (shape[0] > 0 and shape[1] > 0):\n",
        "\t\tface = cv2.resize(face, (32,32))\n",
        "\t\tif(label == \"real\"):\n",
        "\t\t\tcv2.imwrite(\"/content/drive/MyDrive/liveliness/semiset_faces/jpg/real/\" + split[-1], face)\n",
        "\t\telse:\n",
        "\t\t\tcv2.imwrite(\"/content/drive/MyDrive/liveliness/semiset_faces/jpg/fake/\" + split[-1], face)\n",
        "\t\t# data.append(face)\n",
        "\t\tlabels.append(label)\n",
        "\n",
        "file = open(\"/content/drive/MyDrive/liveliness/semiset_faces/jpg/labels.txt\", \"w\")\n",
        "file.writelines(labels)\n",
        "file.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}